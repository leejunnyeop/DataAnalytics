---
title: "데이터애널리스트_과제4_20213572_이준엽"
author: "이준엽"
date: "2024-05-04"
output: html_document
---

#### 필요한 패키지 로드

```{r warning = FALSE, message = FALSE}

# 필요한 패키지 로드
library(mice)      # 결측치 처리를 위한 패키지
library(ggplot2)   # 시각화를 위한 패키지
library(caret)     # 모델링을 위한 패키지
library(visdat)

```

## 1. 가석방 예측

### <span style="color:indigo">  1. 데이터셋에 결측치가 있는지 확인한다. 시각화를 통해 어떠한 변수에 어느 정도의 결측치가 있는지 확인해보고, imputation 과정을 통해 결측치를 적절한 값으로 대체시킨다. 간단하게 다음과 같이 mice package를 활용하자.  </span>

#### 데이터 로드

```{r warning = FALSE, message = FALSE}
# 데이터 로드
pv <- read.csv("parole_violator.csv", fileEncoding = 'UTF-8')

# 데이터 구조 확인
str(pv)

```



```{r warning = FALSE, message = FALSE}
# 결측치 확인
vis_miss(pv)
```

```{r warning = FALSE, message = FALSE}

# 결측치 처리
set.seed(123) # 재현 가능성을 위해 시드 설정
completed_data <- complete(mice(pv))

```
### <span style="color:indigo"> 2. 그래프를 활용하여 데이터셋을 시각화해보고, 이로부터 변수들의 특성을 분석해보자. </span>



```{r warning = FALSE, message = FALSE}

# 'Violator' 변수의 빈도수를 바 차트로 시각화
ggplot(completed_data, aes(x = Violator)) +
  geom_bar(fill = "green", color = "black") +
  theme_minimal() +
  ggtitle("Bar Chart of Violator Status")



```


```{r warning = FALSE, message = FALSE}

# 'Age'와 'TimeServed' 간의 관계를 산점도로 시각화
ggplot(completed_data, aes(x = Age, y = TimeServed)) +
  geom_point(alpha = 0.6, color = "red") +
  theme_minimal() +
  ggtitle("Scatter Plot of Age vs. Time Served")


```

### <span style="color:indigo"> 3. Stratified samling을 통해 traing Set 과 test set을 70:30비율로 분할한다. 그리고  training set을 활용하여 가석방 조건 위반 여부를 예측하기 위한  로직 회귀 모델 을 수립하자. 이때 모든 특징 변수를 모델에 포함시킨다. </span>


```{r warning = FALSE, message = FALSE}

# 필요한 패키지 로드
library(caret)
library(pROC)
library(vip)
library(ggplot2)

```


```{r warning = FALSE, message = FALSE}

# 데이터셋을 훈련 세트와 테스트 세트로 분할
set.seed(123)
trainIndex <- createDataPartition(completed_data$Violator, p=.7, list=FALSE)
pv_train <- completed_data[trainIndex,]
pv_test <- completed_data[-trainIndex,]



```

### a) 로직 회기분석의 결과를 분석해보자

```{r warning = FALSE, message = FALSE}

# 로지스틱 회귀 모델 구축
model <- glm(Violator ~ ., data = pv_train, family = "binomial")

summary(model)
vip(model)
coef(model)

```
#### 분석 결과

Null deviance와 Residual deviance 값의 차이를 통해 모델이 데이터에 얼마나 잘 맞는지를 평가할 수 있습니다. 두 값의 차이가 크면 클수록 모델이 데이터를 잘 설명하고 있다고 볼 수 있습니다. 이 경우 344.07에서 237.95로 감소하였으므로, 모델이 데이터에 적합하게 작동하고 있음을 나타냅니다.
AIC (Akaike Information Criterion): 모델의 복잡성과 적합도를 동시에 고려한 값으로, 낮을수록 좋습니다. 이 경우 AIC 값은 264.39입니다.

### b) 로직 회귀 분석의 결과롭부터, 중복 범죄로 인한 수감자의 가석방 조건 위반 확률에 대해 어떤한 해석을 할 수 있는가?

#### 분석 결과

MultipleOffenses: 중복 범죄 수감자의 경우 가석방 위반 로그 오즈가 약 1.25 높으며, 이는 통계적으로 매우 유의합니다(p=0.00181). 이는 중복 범죄 수감자가 가석방 기간 중 다시 범죄를 저지를 확률이 더 높음을 나타냅니다.

### c) 로직 회귀 분석의 결과로부터, 루이애지애나 의 수감자의 조건 위반 확률에 대해 어떤한 해석을 할 수 있는가?

#### 분석결과

StateLouisiana: 루이지애나 주의 수감자는 다른 주에 비해 가석방 위반 로그 오즈가 약 0.829 높습니다. 그러나 이 역시 유의하지 않습니다(p=0.146).


### d) (남성, 백인, 40대, kentucky state, 4개월 수감, 12개월 형량, 중범 범죄, 마약 범죄)의 정보를 가진 수감자가 가석방 후 조건을 위반할 확률은 확률은 얼만이가? 

```{r warning = FALSE, message = FALSE}

new_data <- data.frame(Male=1, RaceWhite=1, Age=40, State="Kentucky", TimeServed=4, MaxSentence=12, MultipleOffenses=1, Crime="Drugs")
predicted_prob <- predict(model, newdata=new_data, type="response")
print(predicted_prob)


```
#### 분석결과 

 해당 프로파일을 가진 수감자가 가석방 조건을 위반할 확률이 약 26.18%임을 알수 있습니다

### e) 댜양한 threshold t값에 대해 training set의 가석방 조건 위반 여부를 예측해보자. 이때 t값의 변화에 따른 accuracy, sensitivity, specificity 값을 변화를  그래프로 그려보자 

```{r warning = FALSE, message = FALSE}


# 모델에서 예측 확률 계산
predictions <- predict(model, pv_train, type="response")

# 다양한 임계값 설정
thresholds <- seq(0.1, 0.9, by = 0.1)

# 성능 지표 저장을 위한 데이터 프레임 초기화
performance_metrics <- data.frame(Threshold = thresholds,
                                  Accuracy = rep(0, length(thresholds)),
                                  Sensitivity = rep(0, length(thresholds)),
                                  Specificity = rep(0, length(thresholds)))

set.seed(123)
# 각 임계값에 대해 성능 지표 계산
for (t in thresholds) {
  # 임계값에 따라 예측 클래스 결정
  predicted_classes <- ifelse(predictions > t, 1, 0)
  # 혼동 행렬 생성
  cm <- confusionMatrix(as.factor(predicted_classes), as.factor(pv_train$Violator))
  # 성능 지표 저장
  performance_metrics$Accuracy[performance_metrics$Threshold == t] <- cm$overall['Accuracy']
  performance_metrics$Sensitivity[performance_metrics$Threshold == t] <- cm$byClass['Sensitivity']
  performance_metrics$Specificity[performance_metrics$Threshold == t] <- cm$byClass['Specificity']
}

# 성능 지표에 따른 그래프 그리기
ggplot(performance_metrics, aes(x = Threshold)) +
  geom_line(aes(y = Accuracy, colour = "Accuracy")) +
  geom_line(aes(y = Sensitivity, colour = "Sensitivity")) +
  geom_line(aes(y = Specificity, colour = "Specificity")) +
  labs(title = "Threshold에 따른 모델 성능", x = "Threshold", y = "Performance Metric") +
  scale_colour_manual(name = "Metrics", values = c(Accuracy = "blue", Sensitivity = "red", Specificity = "green")) +
  theme_minimal()

```

#### 분석결과 

정확도 

t 값이 증가 하면서 정확도는 비교적 일정하게 유지 되는 됩니다. 일반적으로 정확도는 모델의 전반적인 예측 성공률을 의미하므로, 임계값의 변화에 크게 영향을 받지 않는 것으로 보입니다.

민감도

t 값이 증가함에 따라 어느 시점에 되면 일정해지는 경향을 보인다. 
실제 양성인 경우들 중 모델이 양성으로 정확하게 예측한 비율을 나타내므로, 임계값이 높아질수록 모델이 양성을 놓치는 경우가 더 많아지는 것을 의미합니다

특이도

임계값이 증가함에 따라 특이도가 급격히 감소합니다.
이는 모델이 실제 음성인 경우들을 양성으로 잘못 분류하는 경우가 임계값이 높아짐에 따라 늘어나고 있음을 의미합니다.
특이도는 실제 음성인 경우를 음성으로 정확히 예측하는 비율인데, 임계값이 증가하면 모델이 너무 보수적으로 되어 음성을 감지하지 못하고 양성으로 잘못 판단하는 경우가 증가합니다.

최종 결론

임계값을 너무 낮게 설정하면 민감도는 높지만 특이도가 낮아질 수 있으며, 너무 높게 설정하면 특이도는 높지만 민감도가 낮아집니다.

최적의 임계값은 민감도와 특이도가 적절한 균형을 이루는 지점 0.3에서 0.5 사이일 가능성이 높습니다. 이 범위에서 모델의 성능이 가장 균형 잡히며, 민감도와 특이도의 급격한 감소가 시작되기 전입니다.


### f) Target의 불균형이 큰 경우, F1 score를 분류모델의 성능지표로 사용할 수 있다. F1 score가 의미하는 것이 무엇인지 찾아보자. 그리고 현재 모델에 대해서 여러 threshold  값에 대한 F1 score를 계산해보자. (F1 score는 confusionMatrix() 함수에 mode=“everything”을 인수로 추가하여 출력할 수 있다.)

```{r warning = FALSE, message = FALSE}

# 모델에서 예측 확률 계산
predictions <- predict(model, pv_train, type="response")

# 다양한 임계값 설정
thresholds <- seq(0.1, 0.9, by = 0.1)

# F1 Score 저장을 위한 데이터 프레임 초기화
f1_scores <- data.frame(Threshold = thresholds, F1_Score = rep(0, length(thresholds)))

# 각 임계값에 대해 F1 Score 계산
for (t in thresholds) {
  predicted_classes <- ifelse(predictions > t, 1, 0)
  cm <- confusionMatrix(as.factor(predicted_classes), as.factor(pv_train$Violator), mode = "everything")
  f1_scores$F1_Score[f1_scores$Threshold == t] <- cm$byClass['F1']
}

# F1 Score에 따른 그래프 그리기
ggplot(f1_scores, aes(x = Threshold, y = F1_Score)) +
  geom_line(color = "blue") +
  labs(title = "Threshold에 따른 F1 Score", x = "Threshold", y = "F1 Score") +
  theme_minimal()

```


#### 분석 결과 

그래프에서 보이듯이, F1 Score는 임계값이 0에서 약 0.5까지 증가함에 따라 상승합니다. 이는 초기에 모델이 양성 클래스를 확실하게 식별하는 능력이 강화되면서 정밀도와 민감도가 모두 향상됨을 나타냅니다.
F1 Score는 약 0.5에서 최대값에 도달한 후, 0.5 이상의 임계값에서는 점차 안정적으로 유지되다가 약간의 감소 추세를 보입니다. 이는 높은 임계값에서 민감도가 감소하는 영향을 받기 때문입니다.

이 데이터와 모델에 대한 최적의 임계값은 약 0.5입니다. 이 임계값에서 모델은 정밀도와 민감도 간 최적의 균형을 달성하여 가장 높은 F1 Score를 기록합니다.


### g) 수감자의 가석방 여부를 결정하는 심사위원회에서 이 모델을 사용하여 가석방 조건 위반 여부를 예측한다고 하자. e), f)의 결과를 바탕으로, 심사위원회의 의사결정을 위해서는 threshold 를 어느 정도의 값으로 사용하는 것이 합리적일지 생각해보자.


#### 분석 결과 

먼저 e) 에서 0.1 부터 0.9 까지 비교 실시를 해본 결과 0.3 ~.0.5 값에서 

민감도와 특이도의 급격한 변화는 시점인 것을 확인 했다. 또한 f) 에서 F1 Score 했을떄, 0에서 약 0.5까지 증가함에 따라 상승하고 초기에 모델이 양성 클래스를 확실하게 식별하는 능력이 강화되면서 정밀도와 민감도가 모두 향상됨을 나타냄을 알 수 잇었다. 

종합적으로 판단 했을 때, 0.5에서 임계값이 best 인 것 같습니다.

###  4. 3번의 logistic regression model에 Lasso regularization을 적용해본다. Target 변수의 불균형이 크므로 accuracy보다는 AUC를 기준으로 Cross validation의 성능을 평가하고, CV 결과를 바탕으로 가장 적합한 모델을 선택하자.

```{r warning = FALSE, message = FALSE}

# 필요한 라이브러리 불러오기
library(caret)
library(glmnet)
library(pROC)

```

### a) 어떠한 기준으로 모델을 선택하였으며, 최종적으로 모델에 어떠한 변수들이 포함되었는가?  
```{r warning = FALSE, message = FALSE}


# 기존 로지스틱 회귀 모델을 Lasso 정규화로 확장
set.seed(123)
model_lasso <- cv.glmnet(
  as.matrix(pv_train[-ncol(pv_train)]),  # 입력 변수만 사용
  pv_train$Violator,                  # 종속 변수 지정
  family = "binomial",             # 이진 분류를 위한 binomial 옵션
  alpha = 1,                       # Lasso 정규화 적용 (alpha = 1)
  type.measure = "deviance",       # 성능 측정으로 deviance 사용
  nfolds = 10                      # 10-fold 교차 검증
)

```

```{r warning = FALSE, message = FALSE}


# 최적의 람다 선택
best_lambda <- model_lasso$lambda.min
cat("Best Lambda:", best_lambda, "\n")

# 최적의 람다 값에서 모든 계수를 확인
coef_lasso <- coef(model_lasso, s = "lambda.min")
coef_lasso_full <- as.matrix(coef_lasso)  # Sparse matrix를 full matrix로 변환

# 모든 계수 출력
print(coef_lasso_full)


```
#### 분석결과

변수 'Male', 'Age', 'State', 'Crime'의 계수가 0으로 나타난 것은 Lasso 정규화가 이 변수들의 영향을 모델에서 제거했다는 것을 의미합니다. 이는 해당 변수들이 타겟 변수 'Violator'를 예측하는 데 유의미한 기여를 하지 않거나, 다른 변수와의 중복성으로 인해 제거된 것일 수 있습니다.

### b) 3번의 logistic regression model과 Lasso를 적용한 model의 성능을 나타내는 ROC Curve를 하나의 그래프로 동시에 시각화하고, AUC값을 비교해 보자. Training set과 Test set에 대해 각각 비교해본다. 이 결과로부터 Lasso regularization의 효과가 있는지 분석해보자.

```{r warning = FALSE, message = FALSE}

# 예측 확률 계산
train_logit_probs <- predict(model, pv_train, type="response")
test_logit_probs <- predict(model, pv_test, type="response")
train_lasso_probs <- predict(model_lasso, newx = as.matrix(pv_train[-ncol(pv_train)]), s = "lambda.min")
test_lasso_probs <- predict(model_lasso, newx = as.matrix(pv_test[-ncol(pv_test)]), s = "lambda.min")

# 훈련 세트와 테스트 세트의 ROC 곡선 및 AUC 계산
train_logit_roc <- roc(pv_train$Violator, train_logit_probs)
train_lasso_roc <- roc(pv_train$Violator, train_lasso_probs)
test_logit_roc <- roc(pv_test$Violator, test_logit_probs)
test_lasso_roc <- roc(pv_test$Violator, test_lasso_probs)

# ROC 곡선 시각화
plot(train_logit_roc, col = "blue", main = "ROC Curves - Training and Test Sets")
plot(train_lasso_roc, col = "red", add = TRUE)
plot(test_logit_roc, col = "green", add = TRUE)
plot(test_lasso_roc, col = "orange", add = TRUE)
legend("bottomright", legend = c("Train Logistic", "Train Lasso", "Test Logistic", "Test Lasso"), col = c("blue", "red", "green", "orange"), lwd = 2)

# AUC 값 출력
cat("AUC for Training Set - Logistic Regression:", auc(train_logit_roc), "\n")
cat("AUC for Training Set - Lasso:", auc(train_lasso_roc), "\n")
cat("AUC for Test Set - Logistic Regression:", auc(test_logit_roc), "\n")
cat("AUC for Test Set - Lasso:", auc(test_lasso_roc), "\n")


```

#### 분석결과

 AUC 값 해석

1.훈련 세트의 AUC
   - 로지스틱 회귀 모델: 0.870889
   - Lasso 모델: 0.6730473
   - 로지스틱 회귀 모델이 훈련 데이터에 대해 상당히 잘 적합되어 있음을 보여줍니다. AUC가 0.87은 매우 좋은 성능을 나타내며, 모델이 대부분의 긍정적 사례와 부정적 사례를 잘 구분하고 있음을 의미합니다.
   
   - 반면, Lasso 모델의 AUC는 0.67로 상대적으로 낮습니다. 이는 Lasso 정규화가 너무 강하게 적용되어 일부 중요한 변수의 영향을 과도하게 줄였을 가능성을 시사합니다. 즉, 모델이 데이터의 복잡성을 충분히 캡처하지 못하는 경우일 수 있습니다.

2. 테스트 세트의 AUC
   - 로지스틱 회귀 모델: 0.8015152
   - Lasso 모델: 0.5974747
   - 테스트 세트에서도 로지스틱 회귀 모델의 AUC가 0.80으로, 일반화 성능이 좋다는 것을 보여줍니다. 이는 훈련 데이터와 테스트 데이터 간의 성능 차이가 크지 않아, 과적합의 우려가 적음을 의미합니다.
   
   - Lasso 모델의 경우 테스트 세트의 AUC가 0.60에 불과하며, 이는 모델이 테스트 데이터에 대해 불충분한 예측 성능을 보이고 있음을 나타냅니다. 이는 Lasso 정규화가 너무 강하게 적용되어, 모델이 필요한 정보를 잃었을 수 있습니다.

결론

- 로지스틱 회귀 모델이 Lasso 모델보다 우수한 성능을 보이고 있습니다. 이는 Lasso 정규화가 이 경우에는 너무 강하게 적용될 수 있음을 의미합니다. Lasso 정규화는 변수 선택과 과적합 방지에 유리하지만, 너무 많은 특성을 제거하면 모델의 예측 능력이 저하될 수 있습니다.



### <span style="color:indigo"> 5. 마지막으로 SVM을 적용해보자. Linear, polynomial, RBF kernel들을 사용하여 SVM 모델을 만들어본다. CV를 활용한 parameter tuning을 통해 좋은 성능의 모델을 찾아보자. </span>

```{r warning = FALSE, message = FALSE}

# 필요한 라이브러리 불러오기
library(caret)
library(e1071)
library(pROC)

```


```{r warning = FALSE, message = FALSE}


#선형 svm
set.seed(123)
svmLine <- tune(svm, Violator ~ ., data = pv_train, kernel = "linear",
                 ranges = list(cost = 10^(-2:2))
)

summary(svmLine)


```
#### 분석결과

최적의 cost 값이 1로 결정되었다는 것이고 모모델이 훈련 데이터에 대해 약 11%의 오류율을 보였다는 것을 나타냅니다

```{r warning = FALSE, message = FALSE}


#RBF 컨널 svm
set.seed(123)
svmRBF <-tune(svm, Violator ~ ., data = pv_train, kernel = "radial",
                 ranges = list(cost = c(0.01, 0.1, 1, 10, 100, 1000),
                               gamma = c(0.01, 0.1, 1, 10, 100))
)

summary(svmRBF)


```

#### 분석 결과

최적의 cost, gamma 값이 1, 1로 결정되었다는 것이고 모델이 훈련 데이터에 대해 약 9.4%의 오류율을 보였다는 것을 나타냅니다



```{r warning = FALSE, message = FALSE}


#다항식 svm
set.seed(123)
svmPolyn <- tune(svm, Violator ~ ., data = pv_train, kernel = "polynomial",
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               degree =c(2, 3, 4))
)


summary(svmPolyn)


```
#### 분석결과 


최적의 cost, degree 값이 1, 4로 결정되었다는 것이고 모모델이 훈련 데이터에 대해 약 10.07%의 오류율을 보였다는 것을 나타냅니다

결론

 세 가지 유형의 SVM 모델 (선형, RBF, 다항식) 각각의 성능 결과에 따르면, RBF 커널 SVM이 가장 낮은 오류율(약 9.43%)을 보여주며, 이는 그 모델이 데이터셋에 가장 효과적으로 적합되고 있음을 나타냅니다. 반면, 다항식 SVM은 오류율이 약 10.07%, 선형 SVM은 오류율이 11.00%로 나타났습니다. 이러한 결과를 바탕으로, RBF 커널 SVM이 현재까지 평가된 모델 중 가장 우수한 성능을 보이는 것으로 결론지을 수 있습니다.

### b) 위 모델의 training set과 test set에 대한 성능을 평가해보자. 본 예측 문제에 대해 SVM 모델이 logistic regression 모델에 비해 우수하다고 할 수 있는가?

```{r warning = FALSE, message = FALSE}

# 레벨 동기화
pv_train$Violator <- factor(pv_train$Violator)
pv_test$Violator <- factor(pv_test$Violator, levels = levels(pv_train$Violator))

# 최적 파라미터로 모델 학습
bestModel <- svm(Violator ~ ., data = pv_train, kernel = "radial",
                 cost = svmRBF$best.parameters$cost,
                 gamma = svmRBF$best.parameters$gamma)

# 성능 평가
predictions_train <- predict(bestModel,  pv_train)
predictions_test <- predict(bestModel,  pv_test)
predictions_train <- factor(predictions_train, levels = levels(pv_train$Violator))
predictions_test <- factor(predictions_test, levels = levels(pv_test$Violator))

confMatrix_train <- confusionMatrix(predictions_train, pv_train$Violator)
confMatrix_test <- confusionMatrix(predictions_test, pv_test$Violator)

# 결과 출력
print(confMatrix_train)
print(confMatrix_test)

```


```{r warning = FALSE, message = FALSE}


# 훈련 세트에서의 ROC 곡선 및 AUC 계산
roc_train <- roc(response = pv_train$Violator, predictor = as.numeric(predictions_train))
auc_train <- auc(roc_train)

# 테스트 세트에서의 ROC 곡선 및 AUC 계산
roc_test <- roc(response = pv_test$Violator, predictor = as.numeric(predictions_test))
auc_test <- auc(roc_test)

# 결과 출력
print(paste("AUC for Training Set:", auc_train))
print(paste("AUC for Testing Set:", auc_test))


```



#### 분석결과



1. 훈련 세트의 AUC:
   -  로지스틱 회귀: 0.870889
   -  Lasso: 0.6730473
   -  SVM (RBF Kernel): 0.875

2. 테스트 세트의 AUC:
   - 로지스틱 회귀: 0.8015152
   - Lasso: 0.5974747
   - SVM (RBF Kernel): 0.53989898989899

- 훈련 세트에서는 SVM (RBF Kernel) 모델이 가장 높은 AUC 값을 보여주며, 로지스틱 회귀 모델과 비슷한 성능을 나타냅니다.

- 테스트 세트에서는 로지스틱 회귀 모델이 가장 높은 AUC를 보여주며, SVM 모델의 성능이 현저히 감소합니다. 

### c) Target 변수의 불균형이 큰 경우 SVM의 성능이 좋지 않을 수 있는가? 만약 그렇다면 원인이 무엇일지 생각해보자.

결정 경계의 편향: SVM은 결정 경계를 설정하여 데이터 포인트를 분류합니다. 데이터셋에 불균형이 있을 때, 모델은 다수의 데이터 포인트를 올바르게 분류하는 것을 목표로 결정 경계를 학습하므로, 소수 클래스의 데이터 포인트가 잘못 분류될 가능성이 높아집니다.

서포트 벡터의 선택: SVM에서 결정 경계를 정의하는 데 중요한 역할을 하는 것은 서포트 벡터입니다. 다수 클래스의 데이터 포인트가 서포트 벡터로 선택될 확률이 높기 때문에, 모델이 소수 클래스를 무시하는 경향이 생길 수 있습니다.

과적합: 다수 클래스에 과도하게 적합되어, 일반화 능력이 떨어질 수 있습니다. 즉, 훈련 데이터에는 잘 작동하지만, 새로운 데이터나 테스트 세트에 대해서는 제대로 작동하지 않을 수 있습니다.